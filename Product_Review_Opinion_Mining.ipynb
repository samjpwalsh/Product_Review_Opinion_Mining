{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Product Review Opinion Mining\n",
    "\n",
    "_Assignment for the University of Bath as part of MSc in Artificial Intelligence_\n",
    "\n",
    "_Data source: Minqing Hu and Bing Liu, 2004. Department of Computer Sicence - University of Illinois at Chicago. (see readme.md in data folders for additional information)_\n",
    "\n",
    "## 1. Task and Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of opinion mining for product reviews consists of two seperate but related challenges:\n",
    "\n",
    "1. Feature Extraction\n",
    "2. Sentiment Detection\n",
    "\n",
    "The feature extraction task is concerned with defining which product features are being described in the reviews and additionally extracting sentiment bearing words. The sentiment detection task is then concerned with determining the polarity of reviews (whether sentiment is positive or negative towards the extracted features).\n",
    "\n",
    "The 17 text files provided follow broadly the same structure, with review sentences being divided by with the characters '##'. Text before these characters denotes annotations which are used as the \"Gold Standard\" product features and sentiment for this analysis. Text after these characters denotes the review sentences themselves. In this analysis I consider the following information from the files:\n",
    "- The Gold Standard product features\n",
    "- The Gold Standard sentiment attached to the product features (though only the polarity, positive or negative)\n",
    "- The sentence itself from which features and sentiment bearing words are extracted, and sentiment detected\n",
    "\n",
    "I will proceed with the analysis step by step using a single text file (Apex AD2600 Progressive-scan DVD player) as a demonstration, with evaluation of feature extraction and sentiment detection conducted across all files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the relevant libraries have been imported, the first step in the process is to load the text file and parse it's contents into a dataframe, with the key task being to seperate the Gold Standard features and sentiments from the sentences themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw text</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Gold Standard</th>\n",
       "      <th>Gold Standard Sentiment Score</th>\n",
       "      <th>Gold Standard Feature</th>\n",
       "      <th>Gold Standard Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##repost from january 13 , 2004 with a better ...</td>\n",
       "      <td>repost from january 13 , 2004 with a better fi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>##does your apex dvd player only play dvd audi...</td>\n",
       "      <td>does your apex dvd player only play dvd audio ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>##or does it play audio and video but scrollin...</td>\n",
       "      <td>or does it play audio and video but scrolling ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##before you try to return the player or waste...</td>\n",
       "      <td>before you try to return the player or waste h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>##no picture : \\n</td>\n",
       "      <td>no picture :</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>dvd player[+3]##i am really impressed by this ...</td>\n",
       "      <td>i am really impressed by this dvd player .</td>\n",
       "      <td>[dvd player[+3]]</td>\n",
       "      <td>[+3]]</td>\n",
       "      <td>[dvd player]</td>\n",
       "      <td>[pos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>##if it can fit in the drive bay , this dvd pl...</td>\n",
       "      <td>if it can fit in the drive bay , this dvd play...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>play[+2], dvd[+2]##for instance , i made sever...</td>\n",
       "      <td>for instance , i made several back-ups of my d...</td>\n",
       "      <td>[play[+2],  dvd[+2]]</td>\n",
       "      <td>[+2], +2]]</td>\n",
       "      <td>[play,  dvd]</td>\n",
       "      <td>[pos, pos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>format[+3]##no matter the format . \\n</td>\n",
       "      <td>no matter the format .</td>\n",
       "      <td>[format[+3]]</td>\n",
       "      <td>[+3]]</td>\n",
       "      <td>[format]</td>\n",
       "      <td>[pos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>##awesome ! \\n</td>\n",
       "      <td>awesome !</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>739 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Raw text  \\\n",
       "0    ##repost from january 13 , 2004 with a better ...   \n",
       "1    ##does your apex dvd player only play dvd audi...   \n",
       "2    ##or does it play audio and video but scrollin...   \n",
       "3    ##before you try to return the player or waste...   \n",
       "4                                    ##no picture : \\n   \n",
       "..                                                 ...   \n",
       "734  dvd player[+3]##i am really impressed by this ...   \n",
       "735  ##if it can fit in the drive bay , this dvd pl...   \n",
       "736  play[+2], dvd[+2]##for instance , i made sever...   \n",
       "737              format[+3]##no matter the format . \\n   \n",
       "738                                     ##awesome ! \\n   \n",
       "\n",
       "                                              Sentence         Gold Standard  \\\n",
       "0    repost from january 13 , 2004 with a better fi...                    []   \n",
       "1    does your apex dvd player only play dvd audio ...                    []   \n",
       "2    or does it play audio and video but scrolling ...                    []   \n",
       "3    before you try to return the player or waste h...                    []   \n",
       "4                                        no picture :                     []   \n",
       "..                                                 ...                   ...   \n",
       "734        i am really impressed by this dvd player .       [dvd player[+3]]   \n",
       "735  if it can fit in the drive bay , this dvd play...                    []   \n",
       "736  for instance , i made several back-ups of my d...  [play[+2],  dvd[+2]]   \n",
       "737                            no matter the format .           [format[+3]]   \n",
       "738                                         awesome !                     []   \n",
       "\n",
       "    Gold Standard Sentiment Score Gold Standard Feature  \\\n",
       "0                              []                    []   \n",
       "1                              []                    []   \n",
       "2                              []                    []   \n",
       "3                              []                    []   \n",
       "4                              []                    []   \n",
       "..                            ...                   ...   \n",
       "734                         [+3]]          [dvd player]   \n",
       "735                            []                    []   \n",
       "736                    [+2], +2]]          [play,  dvd]   \n",
       "737                         [+3]]              [format]   \n",
       "738                            []                    []   \n",
       "\n",
       "    Gold Standard Sentiment  \n",
       "0                        []  \n",
       "1                        []  \n",
       "2                        []  \n",
       "3                        []  \n",
       "4                        []  \n",
       "..                      ...  \n",
       "734                   [pos]  \n",
       "735                      []  \n",
       "736              [pos, pos]  \n",
       "737                   [pos]  \n",
       "738                      []  \n",
       "\n",
       "[739 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please change filepath if re-running code\n",
    "\n",
    "with open('C:/Users/samjp/OneDrive/Desktop/Products/Apex AD2600 Progressive-scan DVD player.txt') as txt_file:\n",
    "    df = pd.DataFrame({'Raw text': txt_file.readlines()})\n",
    "\n",
    "df['Sentence'] = df['Raw text'].str.split('##').str[1]\n",
    "df['Sentence'] = df['Sentence'].str.replace('\\n', \"\")\n",
    "df['Gold Standard'] = df['Raw text'].str.split('##').str[0]\n",
    "df['Gold Standard'] = df['Gold Standard'].str.split(',')\n",
    "df['Gold Standard Sentiment Score'] = [[] for _ in range(len(df))]\n",
    "df['Gold Standard Feature'] = [[] for _ in range(len(df))]\n",
    "df['Gold Standard Sentiment'] = [[] for _ in range(len(df))]\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "for i in range(len(df['Gold Standard'])):\n",
    "    if df.loc[i, 'Gold Standard'] != ['']:\n",
    "        for item in df.loc[i, 'Gold Standard']:\n",
    "            if \"[\" in item:\n",
    "                df.loc[i, 'Gold Standard Sentiment Score'].append(item.split(\"[\")[1])\n",
    "                df.loc[i, 'Gold Standard Feature'].append(item.split(\"[\")[0])\n",
    "\n",
    "for i in range(len(df['Gold Standard Sentiment Score'])):\n",
    "    if df.loc[i, 'Gold Standard Sentiment Score'] != []:\n",
    "        for item in df.loc[i, 'Gold Standard Sentiment Score']:\n",
    "            if \"+\" in item:\n",
    "                df.loc[i, 'Gold Standard Sentiment'].append(\"pos\")\n",
    "            elif \"-\" in item:\n",
    "                df.loc[i, 'Gold Standard Sentiment'].append(\"neg\")\n",
    "            else:\n",
    "                df.loc[i, 'Gold Standard Sentiment'].append(\"neutral\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sentences have multiple features and associated sentiments which are parsed as lists into the \"Gold Standard Feature\" and \"Gold Standard Sentiment\" columns respectively.\n",
    "\n",
    "Here I have also removed the rows which do not contain \"sentences\" for the purposes of the analysis (ie those that do not contain \"##\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Pre-processing and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I look to extract product features and sentiment-bearing words from the \"Sentence\" column. Pre-processing is performed first to standardise the data as much as possible. During this step, several procedures are performed:\n",
    "\n",
    "- The sentence is converted to lowercase - this helps in all future steps of the process, as it ensures that if two words are the same, they will be counted as such, regardless of the case used.\n",
    "- The sentence is tokenized using Spacy's NLP pipeline to seperate out individual parts-of-speech.\n",
    "- Punctuation is removed as it is not likely to be a part of a product feature nor be indicative of sentiment. There may be some exceptions to this, for example, exlamation marks could indicate enthusiasm, but they could equally indicate anger. I have chosen to focus on the words themselves in this analysis as they are more likely to clearly indicate either positive or negative sentiment.\n",
    "- Stopwords are also removed - while they are required to make a sentence readable, they are not likely to either be product features (generally nouns) or sentiment-bearing words (generally adjectives).\n",
    "- Words are lemmatised - again this is to help standardise the words to be analysed so product features and sentiment can more easily be detected. Lemmatisation rather than stemming is more appropriate here as it is important that full (not stemmed) words are extracted as product features to aid readability in the product and feature summaries. \n",
    "\n",
    "The pre-processed sentence is then stored in the \"Cleaned Sentence\" column.\n",
    "\n",
    "\n",
    "While pre-processing and Feature Extraction are seperate steps, they are combined in the code here for efficiency.\n",
    "Product Features are identified and extracted to the \"Identified features\" column in the following way:\n",
    "\n",
    "- Nouns are extracted as potential product features\n",
    "- Spacy's chunking algorithm is applied to identify noun-phrases\n",
    "- Observing that the vast majority of product features within the text files are one or two word phrases. Noun phrases are parsed using nltk's bigram function, which divides the noun phrase into 2-word chunks.\n",
    "- These bigrams are then added to the list of nouns as potential product features occuring in a given sentence.\n",
    "\n",
    "Adjectives are also extracted and stored in the \"Sentiment Words\" column as the most likely parts of speech to be sentiment bearing in relation to features (nouns).\n",
    "\n",
    "Here I chose to extract adjectives before applying an algorithm to determine sentence sentiment. From observations in the data, it is clear that although many sentences contain possible features, it is only a subset of these that display sentiment towards those features. Therefore, it is necessary to extract words that are likely to determine sentiment so the algorithm can be applied selectively to those sentences, reducing the possibility that sentences are labelled as having positive or negative sentiment where no sentiment is actually present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw text</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Gold Standard</th>\n",
       "      <th>Gold Standard Sentiment Score</th>\n",
       "      <th>Gold Standard Feature</th>\n",
       "      <th>Gold Standard Sentiment</th>\n",
       "      <th>Cleaned Sentence</th>\n",
       "      <th>Identified Features</th>\n",
       "      <th>Sentiment Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##repost from january 13 , 2004 with a better ...</td>\n",
       "      <td>repost from january 13 , 2004 with a better fi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>repost january 13 2004 well fit title</td>\n",
       "      <td>[repost, well fit, fit title]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>##does your apex dvd player only play dvd audi...</td>\n",
       "      <td>does your apex dvd player only play dvd audio ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>apex dvd player play dvd audio video</td>\n",
       "      <td>[player, audio, video, apex dvd, dvd player, d...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>##or does it play audio and video but scrollin...</td>\n",
       "      <td>or does it play audio and video but scrolling ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>play audio video scroll black white</td>\n",
       "      <td>[scroll, white, audio video, video scroll, scr...</td>\n",
       "      <td>[audio, black]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##before you try to return the player or waste...</td>\n",
       "      <td>before you try to return the player or waste h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>try return player waste hour call apex tech su...</td>\n",
       "      <td>[return, player, waste, hour, call, support, p...</td>\n",
       "      <td>[simple, troubleshooting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>##no picture : \\n</td>\n",
       "      <td>no picture :</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>picture</td>\n",
       "      <td>[picture, picture]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>dvd player[+3]##i am really impressed by this ...</td>\n",
       "      <td>i am really impressed by this dvd player .</td>\n",
       "      <td>[dvd player[+3]]</td>\n",
       "      <td>[+3]]</td>\n",
       "      <td>[dvd player]</td>\n",
       "      <td>[pos]</td>\n",
       "      <td>impress dvd player</td>\n",
       "      <td>[player, dvd player]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>##if it can fit in the drive bay , this dvd pl...</td>\n",
       "      <td>if it can fit in the drive bay , this dvd play...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>fit drive bay dvd player play</td>\n",
       "      <td>[player, fit drive, drive bay, bay dvd, dvd pl...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>play[+2], dvd[+2]##for instance , i made sever...</td>\n",
       "      <td>for instance , i made several back-ups of my d...</td>\n",
       "      <td>[play[+2],  dvd[+2]]</td>\n",
       "      <td>[+2], +2]]</td>\n",
       "      <td>[play,  dvd]</td>\n",
       "      <td>[pos, pos]</td>\n",
       "      <td>instance up dvd movie dvd r w + r w play dvds</td>\n",
       "      <td>[instance, w, play, dvds, instance, dvd movie,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>format[+3]##no matter the format . \\n</td>\n",
       "      <td>no matter the format .</td>\n",
       "      <td>[format[+3]]</td>\n",
       "      <td>[+3]]</td>\n",
       "      <td>[format]</td>\n",
       "      <td>[pos]</td>\n",
       "      <td>matter format</td>\n",
       "      <td>[format, matter format]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>##awesome ! \\n</td>\n",
       "      <td>awesome !</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>awesome</td>\n",
       "      <td>[]</td>\n",
       "      <td>[awesome]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>739 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Raw text  \\\n",
       "0    ##repost from january 13 , 2004 with a better ...   \n",
       "1    ##does your apex dvd player only play dvd audi...   \n",
       "2    ##or does it play audio and video but scrollin...   \n",
       "3    ##before you try to return the player or waste...   \n",
       "4                                    ##no picture : \\n   \n",
       "..                                                 ...   \n",
       "734  dvd player[+3]##i am really impressed by this ...   \n",
       "735  ##if it can fit in the drive bay , this dvd pl...   \n",
       "736  play[+2], dvd[+2]##for instance , i made sever...   \n",
       "737              format[+3]##no matter the format . \\n   \n",
       "738                                     ##awesome ! \\n   \n",
       "\n",
       "                                              Sentence         Gold Standard  \\\n",
       "0    repost from january 13 , 2004 with a better fi...                    []   \n",
       "1    does your apex dvd player only play dvd audio ...                    []   \n",
       "2    or does it play audio and video but scrolling ...                    []   \n",
       "3    before you try to return the player or waste h...                    []   \n",
       "4                                        no picture :                     []   \n",
       "..                                                 ...                   ...   \n",
       "734        i am really impressed by this dvd player .       [dvd player[+3]]   \n",
       "735  if it can fit in the drive bay , this dvd play...                    []   \n",
       "736  for instance , i made several back-ups of my d...  [play[+2],  dvd[+2]]   \n",
       "737                            no matter the format .           [format[+3]]   \n",
       "738                                         awesome !                     []   \n",
       "\n",
       "    Gold Standard Sentiment Score Gold Standard Feature  \\\n",
       "0                              []                    []   \n",
       "1                              []                    []   \n",
       "2                              []                    []   \n",
       "3                              []                    []   \n",
       "4                              []                    []   \n",
       "..                            ...                   ...   \n",
       "734                         [+3]]          [dvd player]   \n",
       "735                            []                    []   \n",
       "736                    [+2], +2]]          [play,  dvd]   \n",
       "737                         [+3]]              [format]   \n",
       "738                            []                    []   \n",
       "\n",
       "    Gold Standard Sentiment  \\\n",
       "0                        []   \n",
       "1                        []   \n",
       "2                        []   \n",
       "3                        []   \n",
       "4                        []   \n",
       "..                      ...   \n",
       "734                   [pos]   \n",
       "735                      []   \n",
       "736              [pos, pos]   \n",
       "737                   [pos]   \n",
       "738                      []   \n",
       "\n",
       "                                      Cleaned Sentence  \\\n",
       "0               repost january 13 2004 well fit title    \n",
       "1                apex dvd player play dvd audio video    \n",
       "2                 play audio video scroll black white    \n",
       "3    try return player waste hour call apex tech su...   \n",
       "4                                             picture    \n",
       "..                                                 ...   \n",
       "734                                impress dvd player    \n",
       "735                     fit drive bay dvd player play    \n",
       "736     instance up dvd movie dvd r w + r w play dvds    \n",
       "737                                     matter format    \n",
       "738                                           awesome    \n",
       "\n",
       "                                   Identified Features  \\\n",
       "0                        [repost, well fit, fit title]   \n",
       "1    [player, audio, video, apex dvd, dvd player, d...   \n",
       "2    [scroll, white, audio video, video scroll, scr...   \n",
       "3    [return, player, waste, hour, call, support, p...   \n",
       "4                                   [picture, picture]   \n",
       "..                                                 ...   \n",
       "734                               [player, dvd player]   \n",
       "735  [player, fit drive, drive bay, bay dvd, dvd pl...   \n",
       "736  [instance, w, play, dvds, instance, dvd movie,...   \n",
       "737                            [format, matter format]   \n",
       "738                                                 []   \n",
       "\n",
       "               Sentiment Words  \n",
       "0                           []  \n",
       "1                           []  \n",
       "2               [audio, black]  \n",
       "3    [simple, troubleshooting]  \n",
       "4                           []  \n",
       "..                         ...  \n",
       "734                         []  \n",
       "735                         []  \n",
       "736                         []  \n",
       "737                         []  \n",
       "738                  [awesome]  \n",
       "\n",
       "[739 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower case\n",
    "df['Sentence'] = df['Sentence'].str.lower()\n",
    "\n",
    "# remove stopwords & punctuation, lemmatise + Extract Nouns, NPs and Adjectives\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "df['Cleaned Sentence'] = ''\n",
    "df['Identified Features'] = [[] for _ in range(len(df))]\n",
    "df['Sentiment Words'] = [[] for _ in range(len(df))]\n",
    "for i in range(len(df['Sentence'])):\n",
    "    doc = nlp(df.loc[i, 'Sentence'])\n",
    "    cleaned_doc = \"\"\n",
    "    for token in doc:\n",
    "        if token.is_stop is False and token.is_punct is False:\n",
    "            cleaned_doc += str(token.lemma_) + \" \"\n",
    "    df.loc[i, 'Cleaned Sentence'] = cleaned_doc\n",
    "    cleaned_doc = nlp(cleaned_doc)\n",
    "    for token in cleaned_doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            df.loc[i, 'Identified Features'].append(str(token))\n",
    "        if token.pos_ == 'ADJ':\n",
    "            df.loc[i, 'Sentiment Words'].append(str(token))\n",
    "    for chunk in cleaned_doc.noun_chunks:\n",
    "        nltk_tokens = nltk.word_tokenize(str(chunk))\n",
    "        if len(nltk_tokens) > 2:\n",
    "            bigram_list = list(nltk.bigrams(nltk_tokens))\n",
    "            for bigram in bigram_list:\n",
    "                bigram_string = str(bigram[0]) + \" \" + str(bigram[1])\n",
    "                df.loc[i, 'Identified Features'].append(bigram_string)\n",
    "        else:\n",
    "            df.loc[i, 'Identified Features'].append(str(chunk))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have a list of potential product features for each sentence, I conduct an analysis to determine which are most likely to be genuine product features, and prune those that are not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>player</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dvd player</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>play</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappear</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slim design</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen image</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3/4 screen</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matter format</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2140 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Count\n",
       "player           164\n",
       "dvd player        76\n",
       "play              59\n",
       "work              48\n",
       "problem           47\n",
       "...              ...\n",
       "disappear          1\n",
       "slim design        1\n",
       "screen image       1\n",
       "3/4 screen         1\n",
       "matter format      1\n",
       "\n",
       "[2140 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dict = {}\n",
    "for features in df['Identified Features']:\n",
    "    for feature in features:\n",
    "        if feature_dict.get(feature) is None:\n",
    "            feature_dict[feature] = 1\n",
    "        else:\n",
    "            feature_dict[feature] += 1\n",
    "\n",
    "feature_df = pd.DataFrame.from_dict(feature_dict, orient='index', columns=['Count'])\n",
    "feature_df = feature_df.sort_values(by=\"Count\", ascending=False)\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there are some words/phrases here that are potentially product features, for example \"player\" and \"dvd player\". Some, for example \"problem\" and \"disappear\" are not. Additionally, there are far more product features here than reviews in the database, which makes analysis of frequent features difficult. Pruning some features is therefore required, which I do in several ways, firstly through:\n",
    "\n",
    "- Pointwise mutual information (PMI) - This technique is used to prune redundant bigrams by measuring co-occurance of the two words, ie. the probability that the words occur together (determined by the number of times the a particular bigram appears in the corpus) compared to the number of times the words appear in total. bigrams with low PMI (<-3 in this analysis) are pruned from the feature set.\n",
    "- Infrequent features - A feature (weather a bigram or unigram) is pruned if it appears in <1% of reviews. This eliminates a large portion of features that are not likely contribute to the analysis as they occur in only a small number of review sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features remaining: 56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>PMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>player</th>\n",
       "      <td>164</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dvd player</th>\n",
       "      <td>76</td>\n",
       "      <td>-1.473172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>play</th>\n",
       "      <td>59</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>48</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dvd</th>\n",
       "      <td>47</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit</th>\n",
       "      <td>36</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disc</th>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picture</th>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apex</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thing</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>button</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer</th>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Count       PMI\n",
       "player        164  0.000000\n",
       "dvd player     76 -1.473172\n",
       "play           59  0.000000\n",
       "work           48  0.000000\n",
       "problem        47  0.000000\n",
       "dvd            47  0.000000\n",
       "unit           36  0.000000\n",
       "disc           34  0.000000\n",
       "picture        34  0.000000\n",
       "feature        33  0.000000\n",
       "price          30  0.000000\n",
       "video          27  0.000000\n",
       "month          27  0.000000\n",
       "apex           26  0.000000\n",
       "product        25  0.000000\n",
       "time           24  0.000000\n",
       "money          23  0.000000\n",
       "thing          22  0.000000\n",
       "button         20  0.000000\n",
       "customer       19  0.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redundancy Pruning through pointwise mutual information (if PMI >-3)\n",
    "\n",
    "corpus_size = 0\n",
    "for sentence in df['Cleaned Sentence']:\n",
    "    corpus_size += len(nltk.word_tokenize(sentence))\n",
    "\n",
    "feature_df[\"PMI\"] = 0\n",
    "for index in feature_df.index:\n",
    "    if len(nltk.word_tokenize(index)) == 2:\n",
    "        bigram_count = feature_df.loc[index, \"Count\"]\n",
    "        if index.split(\" \")[0] in feature_df.index:\n",
    "            word_1_count = feature_df.loc[index.split(\" \")[0], \"Count\"]\n",
    "        else:\n",
    "            word_1_count = bigram_count\n",
    "        if \" \" in index:\n",
    "            if index.split(\" \")[1] in feature_df.index:\n",
    "                word_2_count = feature_df.loc[index.split(\" \")[1], \"Count\"]\n",
    "            else:\n",
    "                word_2_count = bigram_count\n",
    "        else:\n",
    "            word_2_count = bigram_count\n",
    "        pmi = math.log(((bigram_count / corpus_size) / ((word_1_count/corpus_size) + (word_2_count/corpus_size))), 2)\n",
    "        feature_df.loc[index, \"PMI\"] = pmi\n",
    "\n",
    "feature_df = feature_df[feature_df.PMI > -3]\n",
    "\n",
    "# Pruning infrequent features (appear in <1% of reviews)\n",
    "\n",
    "feature_df = feature_df[feature_df.Count > 0.01*len(df)]\n",
    "\n",
    "print(f\"Features remaining: {len(feature_df)}\")\n",
    "feature_df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the pruned feature list is applied to the dataframe (df). In doing this, some additional feature pruning takes place at a sentence level. \n",
    "\n",
    "- If a feature in a sentence is a strict subset of another feature in the same sentence, it is removed as a feature in that sentence. For example if the features of one sentence are [dvd, player, dvd player], both dvd and player would be pruned as features from that sentence. This reduces the number of features appearing per sentence where it is likely that they are referring to the same product feature.\n",
    "- As mentioned above, the analysis is only concerned with features where some sentiment is present (which mirrors the text file data), therefore an additional column is added (\"Sentiment Bearing Features\"). For each sentence, this column is populated with the pruned features if and only if adjectives were extracted from the sentence. This column represents the final set of extracted product features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw text</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Gold Standard</th>\n",
       "      <th>Gold Standard Sentiment Score</th>\n",
       "      <th>Gold Standard Feature</th>\n",
       "      <th>Gold Standard Sentiment</th>\n",
       "      <th>Cleaned Sentence</th>\n",
       "      <th>Identified Features</th>\n",
       "      <th>Sentiment Words</th>\n",
       "      <th>Pruned Features</th>\n",
       "      <th>Sentiment Bearing Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##repost from january 13 , 2004 with a better ...</td>\n",
       "      <td>repost from january 13 , 2004 with a better fi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>repost january 13 2004 well fit title</td>\n",
       "      <td>[repost, well fit, fit title]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>##does your apex dvd player only play dvd audi...</td>\n",
       "      <td>does your apex dvd player only play dvd audio ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>apex dvd player play dvd audio video</td>\n",
       "      <td>[player, audio, video, apex dvd, dvd player, d...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[video, apex dvd, dvd player]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>##or does it play audio and video but scrollin...</td>\n",
       "      <td>or does it play audio and video but scrolling ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>play audio video scroll black white</td>\n",
       "      <td>[scroll, white, audio video, video scroll, scr...</td>\n",
       "      <td>[audio, black]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##before you try to return the player or waste...</td>\n",
       "      <td>before you try to return the player or waste h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>try return player waste hour call apex tech su...</td>\n",
       "      <td>[return, player, waste, hour, call, support, p...</td>\n",
       "      <td>[simple, troubleshooting]</td>\n",
       "      <td>[return, player, hour, support]</td>\n",
       "      <td>[return, player, hour, support]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>##no picture : \\n</td>\n",
       "      <td>no picture :</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>picture</td>\n",
       "      <td>[picture, picture]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[picture]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>dvd player[+3]##i am really impressed by this ...</td>\n",
       "      <td>i am really impressed by this dvd player .</td>\n",
       "      <td>[dvd player[+3]]</td>\n",
       "      <td>[+3]]</td>\n",
       "      <td>[dvd player]</td>\n",
       "      <td>[pos]</td>\n",
       "      <td>impress dvd player</td>\n",
       "      <td>[player, dvd player]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[dvd player]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>##if it can fit in the drive bay , this dvd pl...</td>\n",
       "      <td>if it can fit in the drive bay , this dvd play...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>fit drive bay dvd player play</td>\n",
       "      <td>[player, fit drive, drive bay, bay dvd, dvd pl...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[dvd player]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>play[+2], dvd[+2]##for instance , i made sever...</td>\n",
       "      <td>for instance , i made several back-ups of my d...</td>\n",
       "      <td>[play[+2],  dvd[+2]]</td>\n",
       "      <td>[+2], +2]]</td>\n",
       "      <td>[play,  dvd]</td>\n",
       "      <td>[pos, pos]</td>\n",
       "      <td>instance up dvd movie dvd r w + r w play dvds</td>\n",
       "      <td>[instance, w, play, dvds, instance, dvd movie,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[play, dvds]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>format[+3]##no matter the format . \\n</td>\n",
       "      <td>no matter the format .</td>\n",
       "      <td>[format[+3]]</td>\n",
       "      <td>[+3]]</td>\n",
       "      <td>[format]</td>\n",
       "      <td>[pos]</td>\n",
       "      <td>matter format</td>\n",
       "      <td>[format, matter format]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[format]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>##awesome ! \\n</td>\n",
       "      <td>awesome !</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>awesome</td>\n",
       "      <td>[]</td>\n",
       "      <td>[awesome]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>739 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Raw text  \\\n",
       "0    ##repost from january 13 , 2004 with a better ...   \n",
       "1    ##does your apex dvd player only play dvd audi...   \n",
       "2    ##or does it play audio and video but scrollin...   \n",
       "3    ##before you try to return the player or waste...   \n",
       "4                                    ##no picture : \\n   \n",
       "..                                                 ...   \n",
       "734  dvd player[+3]##i am really impressed by this ...   \n",
       "735  ##if it can fit in the drive bay , this dvd pl...   \n",
       "736  play[+2], dvd[+2]##for instance , i made sever...   \n",
       "737              format[+3]##no matter the format . \\n   \n",
       "738                                     ##awesome ! \\n   \n",
       "\n",
       "                                              Sentence         Gold Standard  \\\n",
       "0    repost from january 13 , 2004 with a better fi...                    []   \n",
       "1    does your apex dvd player only play dvd audio ...                    []   \n",
       "2    or does it play audio and video but scrolling ...                    []   \n",
       "3    before you try to return the player or waste h...                    []   \n",
       "4                                        no picture :                     []   \n",
       "..                                                 ...                   ...   \n",
       "734        i am really impressed by this dvd player .       [dvd player[+3]]   \n",
       "735  if it can fit in the drive bay , this dvd play...                    []   \n",
       "736  for instance , i made several back-ups of my d...  [play[+2],  dvd[+2]]   \n",
       "737                            no matter the format .           [format[+3]]   \n",
       "738                                         awesome !                     []   \n",
       "\n",
       "    Gold Standard Sentiment Score Gold Standard Feature  \\\n",
       "0                              []                    []   \n",
       "1                              []                    []   \n",
       "2                              []                    []   \n",
       "3                              []                    []   \n",
       "4                              []                    []   \n",
       "..                            ...                   ...   \n",
       "734                         [+3]]          [dvd player]   \n",
       "735                            []                    []   \n",
       "736                    [+2], +2]]          [play,  dvd]   \n",
       "737                         [+3]]              [format]   \n",
       "738                            []                    []   \n",
       "\n",
       "    Gold Standard Sentiment  \\\n",
       "0                        []   \n",
       "1                        []   \n",
       "2                        []   \n",
       "3                        []   \n",
       "4                        []   \n",
       "..                      ...   \n",
       "734                   [pos]   \n",
       "735                      []   \n",
       "736              [pos, pos]   \n",
       "737                   [pos]   \n",
       "738                      []   \n",
       "\n",
       "                                      Cleaned Sentence  \\\n",
       "0               repost january 13 2004 well fit title    \n",
       "1                apex dvd player play dvd audio video    \n",
       "2                 play audio video scroll black white    \n",
       "3    try return player waste hour call apex tech su...   \n",
       "4                                             picture    \n",
       "..                                                 ...   \n",
       "734                                impress dvd player    \n",
       "735                     fit drive bay dvd player play    \n",
       "736     instance up dvd movie dvd r w + r w play dvds    \n",
       "737                                     matter format    \n",
       "738                                           awesome    \n",
       "\n",
       "                                   Identified Features  \\\n",
       "0                        [repost, well fit, fit title]   \n",
       "1    [player, audio, video, apex dvd, dvd player, d...   \n",
       "2    [scroll, white, audio video, video scroll, scr...   \n",
       "3    [return, player, waste, hour, call, support, p...   \n",
       "4                                   [picture, picture]   \n",
       "..                                                 ...   \n",
       "734                               [player, dvd player]   \n",
       "735  [player, fit drive, drive bay, bay dvd, dvd pl...   \n",
       "736  [instance, w, play, dvds, instance, dvd movie,...   \n",
       "737                            [format, matter format]   \n",
       "738                                                 []   \n",
       "\n",
       "               Sentiment Words                  Pruned Features  \\\n",
       "0                           []                               []   \n",
       "1                           []    [video, apex dvd, dvd player]   \n",
       "2               [audio, black]                               []   \n",
       "3    [simple, troubleshooting]  [return, player, hour, support]   \n",
       "4                           []                        [picture]   \n",
       "..                         ...                              ...   \n",
       "734                         []                     [dvd player]   \n",
       "735                         []                     [dvd player]   \n",
       "736                         []                     [play, dvds]   \n",
       "737                         []                         [format]   \n",
       "738                  [awesome]                               []   \n",
       "\n",
       "          Sentiment Bearing Features  \n",
       "0                                 []  \n",
       "1                                 []  \n",
       "2                                 []  \n",
       "3    [return, player, hour, support]  \n",
       "4                                 []  \n",
       "..                               ...  \n",
       "734                               []  \n",
       "735                               []  \n",
       "736                               []  \n",
       "737                               []  \n",
       "738                               []  \n",
       "\n",
       "[739 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply pruned features to data\n",
    "\n",
    "df['Pruned Features'] = [[] for _ in range(len(df))]\n",
    "for i in range(len(df['Identified Features'])):\n",
    "    for feature in df.loc[i, 'Identified Features']:\n",
    "        if feature in feature_df.index and feature not in df.loc[i, 'Pruned Features']:\n",
    "            df.loc[i, 'Pruned Features'].append(feature)\n",
    "\n",
    "# Remove those features that are subsets of others per sentence\n",
    "\n",
    "for i in range(len(df['Pruned Features'])):\n",
    "    if len(df.loc[i, 'Pruned Features']) > 1:\n",
    "        for feature in df.loc[i, 'Pruned Features'].copy():\n",
    "            if len(df.loc[i, 'Pruned Features']) > 1:\n",
    "                other_features = df.loc[i, 'Pruned Features'].copy()\n",
    "                other_features.remove(feature)\n",
    "                for other_feature in other_features:\n",
    "                    if feature in other_feature and feature in df.loc[i, 'Pruned Features']:\n",
    "                        df.loc[i, 'Pruned Features'].remove(feature)\n",
    "\n",
    "# Remove features with no sentiment\n",
    "\n",
    "df['Sentiment Bearing Features'] = [[] for _ in range(len(df))]\n",
    "for i in range(len(df['Pruned Features'])):\n",
    "    if df.loc[i, 'Sentiment Words'] != []:\n",
    "        for feature in df.loc[i, 'Pruned Features']:\n",
    "            df.loc[i, 'Sentiment Bearing Features'].append(feature)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the effectiveness of the feature extraction, two metrics are commonly used:\n",
    "- Precision - the proportion of actual features that matched predicted features\n",
    "- Recall - the proportion of the predicted features that matched the actual features\n",
    "These metrics are applied to each of the 17 text files seperately, using the Gold Standard features as a measure of accuracy. The output is generated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is a concatination of the steps above\n",
    "\n",
    "def feature_extraction(file):\n",
    "\n",
    "    with open(file) as txt_file:\n",
    "        df = pd.DataFrame({'Raw text': txt_file.readlines()})\n",
    "\n",
    "    df['Sentence'] = df['Raw text'].str.split('##').str[1]\n",
    "    df['Sentence'] = df['Sentence'].str.replace('\\n', \"\")\n",
    "    df['Gold Standard'] = df['Raw text'].str.split('##').str[0]\n",
    "    df['Gold Standard'] = df['Gold Standard'].str.split(',')\n",
    "    df['Gold Standard Sentiment Score'] = [[] for _ in range(len(df))]\n",
    "    df['Gold Standard Feature'] = [[] for _ in range(len(df))]\n",
    "    df['Gold Standard Sentiment'] = [[] for _ in range(len(df))]\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    for i in range(len(df['Gold Standard'])):\n",
    "        if df.loc[i, 'Gold Standard'] != ['']:\n",
    "            for item in df.loc[i, 'Gold Standard']:\n",
    "                if \"[\" in item:\n",
    "                    df.loc[i, 'Gold Standard Sentiment Score'].append(item.split(\"[\")[1])\n",
    "                    df.loc[i, 'Gold Standard Feature'].append(item.split(\"[\")[0])\n",
    "\n",
    "    for i in range(len(df['Gold Standard Sentiment Score'])):\n",
    "        if df.loc[i, 'Gold Standard Sentiment Score'] != []:\n",
    "            for item in df.loc[i, 'Gold Standard Sentiment Score']:\n",
    "                if \"+\" in item:\n",
    "                    df.loc[i, 'Gold Standard Sentiment'].append(\"pos\")\n",
    "                elif \"-\" in item:\n",
    "                    df.loc[i, 'Gold Standard Sentiment'].append(\"neg\")\n",
    "                else:\n",
    "                    df.loc[i, 'Gold Standard Sentiment'].append(\"neutral\")\n",
    "\n",
    "\n",
    "    # Lower case\n",
    "    df['Sentence'] = df['Sentence'].str.lower()\n",
    "\n",
    "    # remove stopwords & punctuation, lemmatise + Extract Nouns, NPs and Adjectives\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    df['Cleaned Sentence'] = ''\n",
    "    df['Identified Features'] = [[] for _ in range(len(df))]\n",
    "    df['Sentiment Words'] = [[] for _ in range(len(df))]\n",
    "    for i in range(len(df['Sentence'])):\n",
    "        doc = nlp(df.loc[i, 'Sentence'])\n",
    "        cleaned_doc = \"\"\n",
    "        for token in doc:\n",
    "            if token.is_stop is False and token.is_punct is False:\n",
    "                cleaned_doc += str(token.lemma_) + \" \"\n",
    "        df.loc[i, 'Cleaned Sentence'] = cleaned_doc\n",
    "        cleaned_doc = nlp(cleaned_doc)\n",
    "        for token in cleaned_doc:\n",
    "            if token.pos_ == 'NOUN':\n",
    "                df.loc[i, 'Identified Features'].append(str(token))\n",
    "            if token.pos_ == 'ADJ':\n",
    "                df.loc[i, 'Sentiment Words'].append(str(token))\n",
    "        for chunk in cleaned_doc.noun_chunks:\n",
    "            nltk_tokens = nltk.word_tokenize(str(chunk))\n",
    "            if len(nltk_tokens) > 2:\n",
    "                bigram_list = list(nltk.bigrams(nltk_tokens))\n",
    "                for bigram in bigram_list:\n",
    "                    bigram_string = str(bigram[0]) + \" \" + str(bigram[1])\n",
    "                    df.loc[i, 'Identified Features'].append(bigram_string)\n",
    "            else:\n",
    "                df.loc[i, 'Identified Features'].append(str(chunk))\n",
    "\n",
    "    # Build feature list (dataframe)\n",
    "\n",
    "    feature_dict = {}\n",
    "    for features in df['Identified Features']:\n",
    "        for feature in features:\n",
    "            if feature_dict.get(feature) is None:\n",
    "                feature_dict[feature] = 1\n",
    "            else:\n",
    "                feature_dict[feature] += 1\n",
    "\n",
    "    feature_df = pd.DataFrame.from_dict(feature_dict, orient='index', columns=['Count'])\n",
    "    feature_df = feature_df.sort_values(by=\"Count\", ascending=False)\n",
    "\n",
    "    # Redundancy Pruning through pointwise mutual information (if PMI >-3)\n",
    "\n",
    "    corpus_size = 0\n",
    "    for sentence in df['Cleaned Sentence']:\n",
    "        corpus_size += len(nltk.word_tokenize(sentence))\n",
    "\n",
    "    feature_df[\"PMI\"] = 0\n",
    "    for index in feature_df.index:\n",
    "        if len(nltk.word_tokenize(index)) == 2:\n",
    "            bigram_count = feature_df.loc[index, \"Count\"]\n",
    "            if index.split(\" \")[0] in feature_df.index:\n",
    "                word_1_count = feature_df.loc[index.split(\" \")[0], \"Count\"]\n",
    "            else:\n",
    "                word_1_count = bigram_count\n",
    "            if \" \" in index:\n",
    "                if index.split(\" \")[1] in feature_df.index:\n",
    "                    word_2_count = feature_df.loc[index.split(\" \")[1], \"Count\"]\n",
    "                else:\n",
    "                    word_2_count = bigram_count\n",
    "            else:\n",
    "                word_2_count = bigram_count\n",
    "            pmi = math.log(((bigram_count / corpus_size) / ((word_1_count/corpus_size) + (word_2_count/corpus_size))), 2)\n",
    "            feature_df.loc[index, \"PMI\"] = pmi\n",
    "\n",
    "    feature_df = feature_df[feature_df.PMI > -3]\n",
    "\n",
    "    # Pruning infrequent features (appear in <1% of reviews)\n",
    "\n",
    "    feature_df = feature_df[feature_df.Count > 0.01*len(df)]\n",
    "\n",
    "    # Apply pruned features to data\n",
    "\n",
    "    df['Pruned Features'] = [[] for _ in range(len(df))]\n",
    "    for i in range(len(df['Identified Features'])):\n",
    "        for feature in df.loc[i, 'Identified Features']:\n",
    "            if feature in feature_df.index and feature not in df.loc[i, 'Pruned Features']:\n",
    "                df.loc[i, 'Pruned Features'].append(feature)\n",
    "\n",
    "    # Remove those features that are subsets of others per sentence\n",
    "\n",
    "    for i in range(len(df['Pruned Features'])):\n",
    "        if len(df.loc[i, 'Pruned Features']) > 1:\n",
    "            for feature in df.loc[i, 'Pruned Features'].copy():\n",
    "                if len(df.loc[i, 'Pruned Features']) > 1:\n",
    "                    other_features = df.loc[i, 'Pruned Features'].copy()\n",
    "                    other_features.remove(feature)\n",
    "                    for other_feature in other_features:\n",
    "                        if feature in other_feature and feature in df.loc[i, 'Pruned Features']:\n",
    "                            df.loc[i, 'Pruned Features'].remove(feature)\n",
    "\n",
    "    # Remove features with no sentiment\n",
    "\n",
    "    df['Sentiment Bearing Features'] = [[] for _ in range(len(df))]\n",
    "    for i in range(len(df['Pruned Features'])):\n",
    "        if df.loc[i, 'Sentiment Words'] != []:\n",
    "            for feature in df.loc[i, 'Pruned Features']:\n",
    "                df.loc[i, 'Sentiment Bearing Features'].append(feature)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(feature_extraction_dataframe):\n",
    "\n",
    "    true_positive_count = 0 # when i predict the gold standard feature\n",
    "    false_positives_count = 0 # when i predict something which isn't a gold standard feature\n",
    "    false_negative_count = 0 # when i don't predict something which is a gold standard feature\n",
    "\n",
    "    for i in range(len(feature_extraction_dataframe['Sentiment Bearing Features'])):\n",
    "        for feature in feature_extraction_dataframe.loc[i, 'Sentiment Bearing Features']:\n",
    "            if feature in feature_extraction_dataframe.loc[i, 'Gold Standard Feature']:\n",
    "                true_positive_count += 1\n",
    "            elif feature not in feature_extraction_dataframe.loc[i, 'Gold Standard Feature']:\n",
    "                false_positives_count += 1\n",
    "        for feature in feature_extraction_dataframe.loc[i, 'Gold Standard Feature']:\n",
    "            if feature not in feature_extraction_dataframe.loc[i, 'Sentiment Bearing Features']:\n",
    "                false_negative_count += 1\n",
    "\n",
    "    precision = true_positive_count / (true_positive_count + false_positives_count)\n",
    "    recall = true_positive_count / (true_positive_count + false_negative_count)\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Product  Precision    Recall\n",
      "0     Apex AD2600 Progressive-scan DVD player   0.113475  0.149184\n",
      "1                                    Canon G3   0.106061  0.269231\n",
      "2                       Canon PowerShot SD500   0.074742  0.195946\n",
      "3                                  Canon S100   0.109181  0.200913\n",
      "4                                    Computer   0.111853  0.189266\n",
      "5   Creative Labs Nomad Jukebox Zen Xtra 40GB   0.154362  0.298701\n",
      "6                                Diaper Champ   0.077083  0.154812\n",
      "7                              Hitachi router   0.183938  0.267925\n",
      "8                                        ipod   0.038333  0.119792\n",
      "9                              Linksys Router   0.066856  0.212670\n",
      "10                                   MicroMP3   0.072711  0.134106\n",
      "11                         Nikon coolpix 4300   0.155689  0.384236\n",
      "12                                 Nokia 6600   0.153046  0.219616\n",
      "13                                 Nokia 6610   0.200000  0.328402\n",
      "14                                     norton   0.086681  0.167347\n",
      "15                                     Router   0.053571  0.214984\n",
      "16                                    Speaker   0.072967  0.138636\n",
      "Precision    0.107679\n",
      "Recall       0.214457\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-8c059f47ca0e>:48: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  print(precision_recall_df.mean())\n"
     ]
    }
   ],
   "source": [
    "# Please change filepaths if you wish to re-run code\n",
    "\n",
    "filepaths = ['C:/Users/samjp/OneDrive/Desktop/Products/Apex AD2600 Progressive-scan DVD player.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Canon G3.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Canon PowerShot SD500.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Canon S100.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Computer.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Creative Labs Nomad Jukebox Zen Xtra 40GB.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Diaper Champ.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Hitachi router.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/ipod.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Linksys Router.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/MicroMP3.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Nikon coolpix 4300.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Nokia 6600.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Nokia 6610.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/norton.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Router.txt',\n",
    "             'C:/Users/samjp/OneDrive/Desktop/Products/Speaker.txt']\n",
    "\n",
    "filenames = ['Apex AD2600 Progressive-scan DVD player',\n",
    "             'Canon G3',\n",
    "             'Canon PowerShot SD500',\n",
    "             'Canon S100',\n",
    "             'Computer',\n",
    "             'Creative Labs Nomad Jukebox Zen Xtra 40GB',\n",
    "             'Diaper Champ',\n",
    "             'Hitachi router',\n",
    "             'ipod',\n",
    "             'Linksys Router',\n",
    "             'MicroMP3',\n",
    "             'Nikon coolpix 4300',\n",
    "             'Nokia 6600',\n",
    "             'Nokia 6610',\n",
    "             'norton',\n",
    "             'Router',\n",
    "             'Speaker']\n",
    "\n",
    "precision_recall_df = pd.DataFrame(columns=[\"Product\", \"Precision\", \"Recall\"])\n",
    "\n",
    "for i in range(len(filepaths)):\n",
    "    extracted_df = feature_extraction(filepaths[i])\n",
    "    product = filenames[i]\n",
    "    precision, recall = precision_recall(extracted_df)\n",
    "    precision_recall_df.loc[len(precision_recall_df.index)] = [product, precision, recall]\n",
    "    \n",
    "print(precision_recall_df)\n",
    "print(precision_recall_df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that precision and recall is low for all products, with an average precision of 0.11 and recall of 0.21. This may be improved in further analysis by using approaches such as infrequent feature identification and opinion word extraction methods described in Hu and Liu (2004, https://www.aaai.org/Papers/AAAI/2004/AAAI04-119.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that sentiment bearing product features have been extracted from the dataset, I use a supervised learning approach (using sklearn's Multinomial Naive Bayes algorithm) to determine whether sentiment is positive or negative. I use the Gold Standard sentiments provided and the cleaned sentence text to train the model, and apply this at a sentence level (ie not considering differing sentiment in the same sentence for two different product features), which would be an interesting additional approach analysis to be explored in the future. I chose Naive Bayes as it's bag-of-words approach tends to perform well in binary classification tasks, especially sentiment analysis.\n",
    "\n",
    "A quirk of the dataset is that the Gold Standard features and sentiment do not necessarily match the extracted sentiment bearing features (ie some extracted features are not labelled in the data and some labelled data are not extracted). In this case I only consider sentences with both extracted sentiment bearing features, and Gold Standard features and sentiment. This does somewhat reduce the size of the training/testing set, however there is enough overlap to generate a decent size dataset for all products.\n",
    "\n",
    "The approach is applied to each of the 17 text files seperately, with laplace smoothing and an 80%/20% training vs testing data split and accuracy (the proportion of sentence sentiment correctly identified) reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyser_accuracy(feature_extraction_dataframe):\n",
    "\n",
    "    feature_extraction_dataframe[\"Sentence sentiment\"] = \"\"\n",
    "    for i in range(len(feature_extraction_dataframe[\"Gold Standard Sentiment\"])):\n",
    "        if \"pos\" in feature_extraction_dataframe.loc[i, \"Gold Standard Sentiment\"] and \"neg\" not in feature_extraction_dataframe.loc[i, \"Gold Standard Sentiment\"]:\n",
    "            feature_extraction_dataframe.loc[i, \"Sentence sentiment\"] = \"pos\"\n",
    "        elif \"neg\" in feature_extraction_dataframe.loc[i, \"Gold Standard Sentiment\"] and \"pos\" not in feature_extraction_dataframe.loc[i, \"Gold Standard Sentiment\"]:\n",
    "            feature_extraction_dataframe.loc[i, \"Sentence sentiment\"] = \"neg\"\n",
    "\n",
    "    training_df = feature_extraction_dataframe.copy()\n",
    "    training_df = training_df.drop(\n",
    "        [\"Raw text\", \"Sentence\", \"Gold Standard\", \"Gold Standard Sentiment Score\", \"Gold Standard Feature\",\n",
    "         \"Gold Standard Sentiment\", \"Identified Features\", \"Sentiment Words\", \"Pruned Features\",\n",
    "         \"Sentiment Bearing Features\"], axis=1)\n",
    "\n",
    "    training_df['Sentence sentiment'].replace('', np.nan, inplace=True)\n",
    "    training_df = training_df.dropna()\n",
    "    training_df = training_df.reset_index(drop=True)\n",
    "\n",
    "    training_data_size = math.ceil(len(training_df) * 0.8)\n",
    "    training_data = training_df.sample(n=training_data_size, random_state=1) # Random state defined so results can be compared with different approaches\n",
    "    testing_data = training_df.drop(training_data.index)\n",
    "    vectorizer = CountVectorizer()\n",
    "    train_X = vectorizer.fit_transform(training_data[\"Cleaned Sentence\"])\n",
    "    train_X = pd.DataFrame(train_X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    train_y = training_data[\"Sentence sentiment\"]\n",
    "    model = MultinomialNB(alpha=1.0)\n",
    "    model.fit(train_X, train_y)\n",
    "\n",
    "    test_X = vectorizer.transform(testing_data[\"Cleaned Sentence\"])\n",
    "    test_X = pd.DataFrame(test_X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    test_y = testing_data[\"Sentence sentiment\"]\n",
    "    accuracy = model.score(test_X, test_y)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Product  Sentiment Accuracy\n",
      "0     Apex AD2600 Progressive-scan DVD player            0.882353\n",
      "1                                    Canon G3            0.829787\n",
      "2                       Canon PowerShot SD500            0.791667\n",
      "3                                  Canon S100            0.764706\n",
      "4                                    Computer            0.717391\n",
      "5   Creative Labs Nomad Jukebox Zen Xtra 40GB            0.808511\n",
      "6                                Diaper Champ            0.833333\n",
      "7                              Hitachi router            0.578947\n",
      "8                                        ipod            0.903226\n",
      "9                              Linksys Router            0.702703\n",
      "10                                   MicroMP3            0.750000\n",
      "11                         Nikon coolpix 4300            0.741935\n",
      "12                                 Nokia 6600            0.714286\n",
      "13                                 Nokia 6610            0.803922\n",
      "14                                     norton            0.825000\n",
      "15                                     Router            0.787234\n",
      "16                                    Speaker            0.810345\n",
      "Sentiment Accuracy    0.779138\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-926d73b49d52>:10: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  print(sentiment_accuracy_df.mean())\n"
     ]
    }
   ],
   "source": [
    "sentiment_accuracy_df = pd.DataFrame(columns=[\"Product\", \"Sentiment Accuracy\"])\n",
    "\n",
    "for i in range(len(filepaths)):\n",
    "    extracted_df = feature_extraction(filepaths[i])\n",
    "    product = filenames[i]\n",
    "    accuracy = sentiment_analyser_accuracy(extracted_df)\n",
    "    sentiment_accuracy_df.loc[len(sentiment_accuracy_df.index)] = [product, accuracy]\n",
    "    \n",
    "print(sentiment_accuracy_df)\n",
    "print(sentiment_accuracy_df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes provides a moderate degree of accuracy across these datasets at 0.78 on average. This is likely hampered by the number of excluded rows, and could be improved further if a larger dataset were used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that sentiment bearing features have been extracted from the data, and sentiment predicted for each sentence. Summaries can be generated at a product level. In the previous step, I excluded sentiment bearing feature sentences with no corresponding Gold Standard feature and sentiment to correctly report accuracy figures. This model is now applied to all sentiment bearing feature sentences so data summaries can by generated accurately. Importantly, the model is still trained on the same dataset as above and data is vectorised based only on words appearing in cleaned sentences in the training set, so there is no data leakage from other data.\n",
    "\n",
    "Below I print out summaries for the 5 most frequent features for 2 products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyser_outputs(feature_extraction_dataframe):\n",
    "    feature_extraction_dataframe[\"Sentence sentiment\"] = \"\"\n",
    "    for i in range(len(feature_extraction_dataframe[\"Gold Standard Sentiment\"])):\n",
    "        if \"pos\" in feature_extraction_dataframe.loc[i, \"Gold Standard Sentiment\"] and \"neg\" not in \\\n",
    "                feature_extraction_dataframe.loc[i, \"Gold Standard Sentiment\"]:\n",
    "            feature_extraction_dataframe.loc[i, \"Sentence sentiment\"] = \"pos\"\n",
    "        elif \"neg\" in feature_extraction_dataframe.loc[i, \"Gold Standard Sentiment\"] and \"pos\" not in \\\n",
    "                feature_extraction_dataframe.loc[i, \"Gold Standard Sentiment\"]:\n",
    "            feature_extraction_dataframe.loc[i, \"Sentence sentiment\"] = \"neg\"\n",
    "\n",
    "    training_df = feature_extraction_dataframe.copy()\n",
    "    training_df = training_df.drop(\n",
    "        [\"Raw text\", \"Sentence\", \"Gold Standard\", \"Gold Standard Sentiment Score\", \"Gold Standard Feature\",\n",
    "         \"Gold Standard Sentiment\", \"Identified Features\", \"Sentiment Words\", \"Pruned Features\",\n",
    "         \"Sentiment Bearing Features\"], axis=1)\n",
    "\n",
    "    training_df['Sentence sentiment'].replace('', np.nan, inplace=True)\n",
    "    training_df = training_df.dropna()\n",
    "    training_df = training_df.reset_index(drop=True)\n",
    "\n",
    "    training_data_size = math.ceil(len(training_df) * 0.8)\n",
    "    training_data = training_df.sample(n=training_data_size, random_state=1)\n",
    "    vectorizer = CountVectorizer()\n",
    "    train_X = vectorizer.fit_transform(training_data[\"Cleaned Sentence\"])\n",
    "    train_X = pd.DataFrame(train_X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    train_y = training_data[\"Sentence sentiment\"]\n",
    "    model = MultinomialNB(alpha=1.0)\n",
    "    model.fit(train_X, train_y)\n",
    "\n",
    "    output_df = feature_extraction_dataframe.copy()\n",
    "    output_df = output_df.drop(\n",
    "        [\"Raw text\", \"Gold Standard\", \"Gold Standard Sentiment Score\", \"Gold Standard Feature\",\n",
    "         \"Gold Standard Sentiment\", \"Identified Features\", \"Sentiment Words\", \"Pruned Features\"\n",
    "            , \"Sentence sentiment\"], axis=1)\n",
    "\n",
    "    for i in range(len(output_df[\"Sentiment Bearing Features\"])):\n",
    "        if output_df.loc[i, \"Sentiment Bearing Features\"] == []:\n",
    "            output_df.loc[i, \"Sentiment Bearing Features\"] = np.NaN\n",
    "    output_df = output_df.dropna()\n",
    "    output_df = output_df.reset_index(drop=True)\n",
    "\n",
    "    test_X = vectorizer.transform(output_df[\"Cleaned Sentence\"])\n",
    "    test_X = pd.DataFrame(test_X.toarray(), columns=vectorizer.get_feature_names())\n",
    "    output_df[\"Predicted Sentiment\"] = model.predict(test_X)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "def generate_summary(product_name, outputs, top_x_features=5, number_sentences=5):\n",
    "    feature_list = []\n",
    "    pos_review_list = []\n",
    "    neg_review_list = []\n",
    "    pos_sentences = []\n",
    "    neg_sentences = []\n",
    "    for i in range(len(outputs)):\n",
    "        for feature in outputs.loc[i, \"Sentiment Bearing Features\"]:\n",
    "            if feature in feature_list:\n",
    "                if outputs.loc[i, \"Predicted Sentiment\"] == \"pos\":\n",
    "                    pos_review_list[feature_list.index(feature)] += 1\n",
    "                    pos_sentences[feature_list.index(feature)].append(str(outputs.loc[i, \"Sentence\"]))\n",
    "                else:\n",
    "                    neg_review_list[feature_list.index(feature)] += 1\n",
    "                    neg_sentences[feature_list.index(feature)].append(str(outputs.loc[i, \"Sentence\"]))\n",
    "\n",
    "            else:\n",
    "                feature_list.append(feature)\n",
    "                if outputs.loc[i, \"Predicted Sentiment\"] == \"pos\":\n",
    "                    pos_review_list.append(1)\n",
    "                    neg_review_list.append(0)\n",
    "                    pos_sentences.append([outputs.loc[i, \"Sentence\"]])\n",
    "                    neg_sentences.append([])\n",
    "                else:\n",
    "                    neg_review_list.append(1)\n",
    "                    pos_review_list.append(0)\n",
    "                    neg_sentences.append([outputs.loc[i, \"Sentence\"]])\n",
    "                    pos_sentences.append([])\n",
    "\n",
    "    data_tuples = list(zip(feature_list, pos_review_list, neg_review_list, pos_sentences, neg_sentences))\n",
    "    summary_df = pd.DataFrame(data_tuples,\n",
    "                              columns=[\"Features\", \"Positive\", \"Negative\", \"Positive Sentences\", \"Negative Sentences\"])\n",
    "    summary_df[\"Total Reviews\"] = summary_df[\"Positive\"] + summary_df[\"Negative\"]\n",
    "    summary_df = summary_df.sort_values(by=\"Total Reviews\", ascending=False)\n",
    "    summary_df = summary_df.reset_index(drop=True)\n",
    "\n",
    "    print(f\"Product Name: {product_name}\")\n",
    "    for i in range(min(len(summary_df), top_x_features)):\n",
    "        print(\"\\n\")\n",
    "        print(f\"Feature: {summary_df.loc[i, 'Features']}\")\n",
    "        print(\"\\n\")\n",
    "        print(f\"Positive: {summary_df.loc[i, 'Positive']}\")\n",
    "        for j in range(min(summary_df.loc[i, 'Positive'], number_sentences)):\n",
    "            print(f\"- {summary_df.loc[i, 'Positive Sentences'][j]}\")\n",
    "        print(\"\\n\")\n",
    "        print(f\"Negative: {summary_df.loc[i, 'Negative']}\")\n",
    "        for j in range(min(summary_df.loc[i, 'Negative'], number_sentences)):\n",
    "            print(f\"- {summary_df.loc[i, 'Negative Sentences'][j]}\")\n",
    "        print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Name: Creative Labs Nomad Jukebox Zen Xtra 40GB\n",
      "\n",
      "\n",
      "Feature: player\n",
      "\n",
      "\n",
      "Positive: 85\n",
      "- like it 's predecessor , the quickly revised nx , this player boasts a decent size and weight , a relatively-intuitive navigational system that categorizes based on id3 tags , and excellent sound ( widely known to be better than ipod - not surprising considering the number of years creative has been in the audio peripheral business ) . \n",
      "- they player 's interface itself is also very easy to use . \n",
      "- i was a little concerned to be the black sheep buying this player instead of the incredibly overpriced apple i-pod . \n",
      "- much cheaper than i-pod good looking player ( beautiful blue back-lit screen ) if you 've read about the player , some have complained about the lack of a viewing hole for the face when the case is on , but this is good because the face does n't get damaged / scratched fast transfer rate \n",
      "- the creative labs zen xtra has all the features the i-pod has and if you get if from amazon your only going to pay $ 300 for this great player . \n",
      "\n",
      "\n",
      "Negative: 38\n",
      "- yes , the ipod looks in tune with the carefully articulated urban-hipster persona thats become popular as of late , but sure-as-hell is n't worth the $ 200 premium ( unless you are just dying to match your music player with that volkswagen beetle and urban outfitters wardrobe ) . \n",
      "- the player hangs up on file transfer every once-in-a-while , prompting a reset . \n",
      "- first , the cons ... a tad bulky ... not the most asthetic looking player ... and does n't support a folder stucture . \n",
      "- and man ... i had to work to put my chinese songs on the player ... because i had the actual chinese names of the songs as the title .. \n",
      "- i have had this player for 2 weeks , i will be returning it for an exchange in the hopes that the locking problem and scroll wheel are problems with this particular unit . \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature: software\n",
      "\n",
      "\n",
      "Positive: 60\n",
      "- the software is great . \n",
      "- transfering is easy , the software makes everythig pretty easy . \n",
      "- software is easy to use ( although redhat software is better , but costs money ) \n",
      "- all in all this a great player that blows the i-pod away , with easy to use and understand features , software that is simple to understand and use , and a great sound ( which is all that matters in a mp3 player ) . \n",
      "- the main problem with the nomad jukebox zen xtra 30 gb is the software . \n",
      "\n",
      "\n",
      "Negative: 39\n",
      "- despite the instructions insisting that the process was fairly automatic , i had to install the usb drivers and the nomad interface software manually . \n",
      "- deficiencies with zennx are easily overcome with 3rd-party earphones ( $ 20 + ) and software ( $ 25 ). \n",
      "- software - music match jukebox is n't the greatest , the search funtion is n't fast even when accessing it with the hotkey shortcut . \n",
      "- i have windows xp pro and when installing the enclosed software i did receive a message that there was a problem installing the driver . \n",
      "- - included creative software is pretty poor . \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature: music\n",
      "\n",
      "\n",
      "Positive: 45\n",
      "- this is especially useful for those who listen mostly to pop music and store all those short ditties . \n",
      "- that 's important because i have a lot of music . \n",
      "- for me the sound of the music is what is important . \n",
      "- this is a good deal for music and data storage . \n",
      "- this is a great investment for someone like me who enjoys music and hates carrying multiple cds . \n",
      "\n",
      "\n",
      "Negative: 14\n",
      "- yes , the ipod looks in tune with the carefully articulated urban-hipster persona thats become popular as of late , but sure-as-hell is n't worth the $ 200 premium ( unless you are just dying to match your music player with that volkswagen beetle and urban outfitters wardrobe ) . \n",
      "- software - music match jukebox is n't the greatest , the search funtion is n't fast even when accessing it with the hotkey shortcut . \n",
      "- this is great as long as all of your music have id3 tags , and if you 've downloaded your songs off the internet , many probably do n't . \n",
      "- over time , i loaded hundreds of cds onto the computer , and the idea of carrying around all of my music with me became attractive . \n",
      "- if the hard drive on your computer should fail , better have your music backed up somewhere or its gone ! \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature: thing\n",
      "\n",
      "\n",
      "Positive: 28\n",
      "- this thing kicks mighty apple butt ! \n",
      "- this thing lasted well over 14 hours when i played it straight for the first time . \n",
      "- you wo n't appreciate the quality this thing has to offer unless you are using a good set of cans. \n",
      "- other than that this thing is great ... \n",
      "- this thing , while looking pretty cool , is not as sexy as the ipod . \n",
      "\n",
      "\n",
      "Negative: 20\n",
      "- i would have given this thing 4.5 stars but since i can 't , i 'll be sweet and give it the full 5 stars . \n",
      "- the only thing i 'm concerned about is the quality / longevity of the buttons on the thing . \n",
      "- that 's probably a bad thing . \n",
      "- despite the things that i have listed so far , i could live with this item since it saves me from lugging a cd case to work , to the gym , in the car , etc. . \n",
      "- quite simply , the firmware and / or the os that control this thing is not ready for prime time . \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature: mp3 player\n",
      "\n",
      "\n",
      "Positive: 39\n",
      "- i probably would have liked to have a player in something other than silver / metallic ... like the battery adapters on their usb thumbdrive ( muvo nx ) mp3 player models . \n",
      "- i shopped around for a month looking for a good mp3 player , and all i heard was how good the i-pod was . \n",
      "- all in all this a great player that blows the i-pod away , with easy to use and understand features , software that is simple to understand and use , and a great sound ( which is all that matters in a mp3 player ) . \n",
      "- on the subject of headphones , this thing seems like a normal mp3 player when you use the headphones they give you in the box , but invest 30 dollars in a nice pair and the sound quality goes up through the roof ! \n",
      "- if you want a sexy , cool , accessory-availible mp3 player , by all means , get an ipod for a hundred dollars more ( or one for the same price that holds 1/3 of the music ) . \n",
      "\n",
      "\n",
      "Negative: 5\n",
      "- i became interested in getting a mp3 player when i got a new work computer ; while my office bars the addition of any non-work software , it does n't care if you load cds onto the hard drive . \n",
      "- this explains why you see what appears to be such weird pricing on mp3 players . \n",
      "- i 'd like to see how well a small company supports its mp3 player in 2 years when its battery dies as all rechargeable do . \n",
      "- if you buy a creative mp3 player , be sure to get the optional extended warranty . \n",
      "- it 's nothing major , just a bad hard drive , any hard drive mp3 player can have that problem . \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Product Name: Diaper Champ\n",
      "\n",
      "\n",
      "Feature: bag\n",
      "\n",
      "\n",
      "Positive: 28\n",
      "- works great, no odor, and uses regular bags.\n",
      "- the bags never fit right and then it never spun right.\n",
      "- we use the cheap $1.99 13 gallon vanilla scented bags from the grocery store but you can use any bag you have laying around.\n",
      "- i just go to [the store] and purchase tall kitchen bags and use those for the diaper champ.\n",
      "- i am happy with the fact that i can use any bag in there and dont' have to pay out the nose for replacements.\n",
      "\n",
      "\n",
      "Negative: 7\n",
      "- (i never know when the champ is full as there is no way too tell.  even though not particularly aesthetic, how about making the champ clear and see-through?) so, i lift the lid and find a wad of diaper, which i must remove, usually stuck inside the lid itself because the bag is too full.\n",
      "- opening the champ to empty out the bag is a little tricky- i pinched my thumb a few times before i got the hang of it.\n",
      "- as one other reviewer pointed out, it does occasionally take a little effort to push down the \"plunger\" to get the diapers in the pail, even though upon inspection the bag doesn't really look full.\n",
      "- we resorted to using small plastic bags to store the dirty diapers and dumping them in a trash can and using the diaper champ for wet diapers only.\n",
      "- as the bag fills, you may need to flip twice to ensure that the weighted cylinder has completely pushed the diaper into the bag.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature: diaper\n",
      "\n",
      "\n",
      "Positive: 22\n",
      "- save your money invest in plastic bags and dispose of diapers by sealing them in the bags and placing them in the regular garbage.\n",
      "- it's also so easy to use and holds a great deal of diapers.\n",
      "- the diaper champ couldn't be easier to use.\n",
      "- it wasn't much different than putting the diaper in a shopping back and putting it in the regular trash.\n",
      "- as a first-time mom, i got the great idea that cloth would be better, and this was the only diaper disposal that advertised compatibility with cloth diapers.\n",
      "\n",
      "\n",
      "Negative: 7\n",
      "- as she entered into toddlerhood we started having some bad diapers just becasue of her cutting teeth and diet changes.\n",
      "- now it will start smelling if you leave it with diapers in it for an extended period but all of them will!\n",
      "- i'm concerned that as she gets older the #2 diapers will only get worse.\n",
      "- the hole to put the diaper in needs to be larger as i am now using medium diapers (i have a 3 month old) and they're on the brink of being too large to fit no matter how compact i make then and how un-soiled they are.\n",
      "- i second that you just need to fold up the diaper (with wipes inside) neatly, but that is not a big deal.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature: use\n",
      "\n",
      "\n",
      "Positive: 18\n",
      "- we use the cheap $1.99 13 gallon vanilla scented bags from the grocery store but you can use any bag you have laying around.\n",
      "- it's easier and much more convenient to use because it doesn't have clumsy and confusing (not to mention expensive) refills.\n",
      "- 1. it uses regular garbage bags instead of brand refills.\n",
      "- the one thing that sets this product apart from the diaper genie is that this product can use regular garbage bags.\n",
      "- this pail doesn't really keep in odors anymore and we'll probably have to get a new one, but since you can use regular garbage bags (we actually use the bags we get our groceries in because they hang perfectly) its still probably a better deal than the genie.\n",
      "\n",
      "\n",
      "Negative: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature: diaper champ\n",
      "\n",
      "\n",
      "Positive: 17\n",
      "- so far (3 weeks), we've had no problems with the diaper champ at all.\n",
      "- i also sprinkled baking soda in the bottom of the diaper champ to help absorb odors - every once in awhile i just empty the old baking soda out and replace it.\n",
      "- the diaper champ is the best we found!\n",
      "- i just go to [the store] and purchase tall kitchen bags and use those for the diaper champ.\n",
      "- i have noticed an odor (one the i expected!) when unloading the diaper champ but at least you don't smell anything every time you use it.\n",
      "\n",
      "\n",
      "Negative: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Feature: diaper pail\n",
      "\n",
      "\n",
      "Positive: 12\n",
      "- we received this diaper pail as a gift while expecting our second child.\n",
      "- if you are looking for a user-friendly diaper pail that actually does keep the odor contained, than the diaper champ is the one for you!\n",
      "- diaper champ is by far the best diaper pail!\n",
      "- whether you are using disposable or cloth diapers, this is the truly the only diaper pail to get!\n",
      "- we opted to purchase this diaper pail even though my sil gave us her used diaper pail (the other popular pail. one that uses its own brand of bags) for free--and we are very glad we did.\n",
      "\n",
      "\n",
      "Negative: 4\n",
      "- i find the major problem with this item is that if you have a very messy diaper and can't tightly bundle it without a mess, the mess gets all over the module that dumps the diaper and the stink is outside of the diaper pail.\n",
      "- any diaper pail, if you leave it for long enough, is going to smell.\n",
      "- as one other reviewer pointed out, it does occasionally take a little effort to push down the \"plunger\" to get the diapers in the pail, even though upon inspection the bag doesn't really look full.\n",
      "- solid food poop is very stinky and this diaper pail doesn't really contain the smell.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5,7):\n",
    "    extracted_df = feature_extraction(filepaths[i])\n",
    "    product = filenames[i]\n",
    "    outputs = sentiment_analyser_outputs(extracted_df)\n",
    "    generate_summary(product, outputs, top_x_features=5, number_sentences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above can be modified for i in range(0,17) to display summaries for other products. The number of features to display can also be modified with top_x_features (these are always displayed from most frequent to least) and the number of positive and negative sentences to print out per feature can be modified with number_sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report I have presented an approach for extracting sentiment bearing features from raw text, pruning the features with a variety of tools, predicting the sentiment of sentences using supervised learning and providing summary outputs from the data. Throughout I have used the evaluation methods of Precision and Recall for feature extraction, and accuracy for sentiment prediction. \n",
    "\n",
    "In the future I could look to develop this approach further. While I am confident that Naive Bayes is a good choice of algorithm for the task, the feature extraction step could be further refined with additional pruning steps or by merging features with similar meanings, perhaps using semantic modelling. Additionallly a more sophisticated approach for identifying sentiment words could be taken (rather than adjective extraction), for example, dependency parsing could identify references to particular features, which could then be extracted for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
